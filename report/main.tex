\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{natbib}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\frobnorm}[1]{\left\| #1 \right\|_F}
\newcommand{\specnorm}[1]{\left\| #1 \right\|_2}
\newcommand{\nnz}{\mathrm{nnz}}
\newcommand{\BigO}{\mathcal{O}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

% ============================================================================
% DOCUMENT INFO
% ============================================================================
\title{Randomized SVD: Theory, Implementation, and Empirical Analysis of Sketching Methods}
\author{Peter Klivnoy}
\date{December 2025}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle

\begin{abstract}
Randomized algorithms for low-rank matrix approximation have become essential tools in modern data science, offering dramatic speedups over classical deterministic methods. This report provides a comprehensive empirical study of randomized SVD with three families of sketching operators: Gaussian random projections, Subsampled Randomized Fourier/Hadamard Transforms (SRFT/SRHT), and sparse embeddings (CountSketch). We implement each method in both optimized (using BLAS/FFT libraries) and naive pure-Python forms to disentangle algorithmic complexity gains from implementation-level optimizations. Our experiments demonstrate that while structured sketches (SRFT/SRHT) achieve the theoretically predicted $\BigO(mn \log \ell)$ complexity advantages in pure-Python comparisons, highly optimized BLAS routines make Gaussian sketches competitive for practical problem sizes. For sparse matrices, CountSketch provides dramatic speedups scaling with the number of nonzeros rather than matrix dimensions. We also investigate the role of power iterations and oversampling in controlling approximation accuracy, validating theoretical predictions from \citet{halko2011finding}. All experiments are fully reproducible via the accompanying code repository.
\end{abstract}

\tableofcontents
\newpage

% ============================================================================
% SECTION 1: INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:intro}

The singular value decomposition (SVD) is a foundational tool in numerical linear algebra with applications spanning principal component analysis, latent semantic indexing, recommender systems, image compression, and scientific computing. Given a matrix $A \in \R^{m \times n}$, the SVD factorizes it as
\begin{equation}
    A = U \Sigma V^T
\end{equation}
where $U \in \R^{m \times m}$ and $V \in \R^{n \times n}$ are orthogonal matrices and $\Sigma \in \R^{m \times n}$ is diagonal with non-negative entries $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{\min(m,n)} \geq 0$.

Computing the full SVD requires $\BigO(\min(m,n)^2 \cdot \max(m,n))$ operations, which becomes prohibitive for large-scale problems. In many applications, however, we only need the \emph{truncated} or \emph{rank-$k$} SVD, retaining only the top $k$ singular values and corresponding singular vectors. The Eckart-Young theorem guarantees that this truncated SVD provides the best rank-$k$ approximation in both spectral and Frobenius norms.

\subsection{Randomized Low-Rank Approximation}

Randomized algorithms offer a compelling alternative: by using random projections to ``sketch'' the matrix into a lower-dimensional subspace, we can compute approximate low-rank factorizations in $\BigO(mn \log k)$ or even $\BigO(\nnz(A) \cdot k)$ time for sparse matrices, with high probability of achieving near-optimal accuracy \citep{halko2011finding, woodruff2014sketching, martinsson2020randomized}.

The core idea is elegantly simple. To find a rank-$k$ approximation of $A$:
\begin{enumerate}
    \item \textbf{Sketch:} Form $Y = A\Omega$ where $\Omega \in \R^{n \times \ell}$ is a random ``test matrix'' with $\ell = k + p$ columns (where $p$ is a small oversampling parameter).
    \item \textbf{Orthogonalize:} Compute an orthonormal basis $Q$ for the column space of $Y$.
    \item \textbf{Project and factor:} Form the small matrix $B = Q^T A$ and compute its SVD.
    \item \textbf{Reconstruct:} The approximate SVD of $A$ is recovered from $Q$ and the SVD of $B$.
\end{enumerate}

The computational bottleneck is Step 1: forming the sketch $Y = A\Omega$. The choice of sketching matrix $\Omega$ determines both the cost of this step and the quality of the resulting approximation.

\subsection{Research Questions}

This report investigates three fundamental questions:

\begin{enumerate}
    \item \textbf{Algorithmic complexity vs.\ implementation:} Structured sketches (SRFT/SRHT) have better asymptotic complexity than Gaussian sketches. Does this translate to practical speedups, or do highly optimized BLAS libraries close the gap?
    
    \item \textbf{Sparse data:} For sparse matrices, do sparse embeddings like CountSketch provide the theoretically predicted advantages over dense sketching methods?
    
    \item \textbf{Accuracy control:} How do power iterations and oversampling affect approximation quality across different spectral decay profiles?
\end{enumerate}

\subsection{Contributions}

Our main contributions are:
\begin{itemize}
    \item A unified implementation of Gaussian, SRFT, SRHT, and CountSketch operators with both optimized and pure-Python variants.
    \item Systematic benchmarks isolating algorithmic complexity from library optimizations.
    \item Empirical validation of theoretical accuracy bounds across diverse spectral decay profiles.
    \item Practical guidelines for practitioners choosing among sketching methods.
\end{itemize}

% ============================================================================
% SECTION 2: BACKGROUND AND THEORY
% ============================================================================
\section{Background and Theory}
\label{sec:background}

We briefly review the theoretical foundations underlying randomized SVD, focusing on the properties that different sketching matrices must satisfy and their computational costs.

\subsection{The Randomized SVD Algorithm}

Algorithm~\ref{alg:randsvd} presents the basic randomized SVD procedure.

\begin{algorithm}[h]
\caption{Randomized SVD \citep{halko2011finding}}
\label{alg:randsvd}
\begin{algorithmic}[1]
\Require Matrix $A \in \R^{m \times n}$, target rank $k$, oversampling $p$, power iterations $q$
\Ensure Approximate rank-$k$ SVD: $\tilde{U}, \tilde{\Sigma}, \tilde{V}$
\State Set $\ell = k + p$
\State Draw random test matrix $\Omega \in \R^{n \times \ell}$
\State Form sketch $Y = (AA^T)^q A\Omega$ \Comment{Power iteration for accuracy}
\State Compute QR factorization $Y = QR$
\State Form $B = Q^T A \in \R^{\ell \times n}$
\State Compute SVD of small matrix: $B = \hat{U} \Sigma V^T$
\State Set $\tilde{U} = Q\hat{U}$
\State \Return $\tilde{U}, \tilde{\Sigma}, \tilde{V} = V$
\end{algorithmic}
\end{algorithm}

\subsection{Sketching Matrices}

The choice of $\Omega$ critically affects both computational cost and approximation quality.

\subsubsection{Gaussian Random Matrices}

The simplest choice is $\Omega_{ij} \sim \mathcal{N}(0, 1)$ i.i.d.\ Gaussian entries.

\begin{itemize}
    \item \textbf{Complexity:} $\BigO(mn\ell)$ to form $Y = A\Omega$
    \item \textbf{Accuracy:} Optimal among oblivious sketches; well-understood error bounds
    \item \textbf{Implementation:} Leverages highly optimized BLAS (DGEMM)
\end{itemize}

\subsubsection{Subsampled Randomized Fourier Transform (SRFT)}

The SRFT uses structured randomness:
\begin{equation}
    \Omega = \sqrt{\frac{n}{\ell}} D F^* S
\end{equation}
where $D$ is a diagonal matrix of random signs, $F$ is the DFT matrix, and $S$ samples $\ell$ columns uniformly at random.

\begin{itemize}
    \item \textbf{Complexity:} $\BigO(mn \log n)$ using FFT, or $\BigO(mn \log \ell)$ with more sophisticated implementations
    \item \textbf{Accuracy:} Comparable to Gaussian with slightly larger oversampling
    \item \textbf{Implementation:} Uses FFT libraries (FFTW, NumPy FFT)
\end{itemize}

\subsubsection{Subsampled Randomized Hadamard Transform (SRHT)}

The SRHT replaces the DFT with the Walsh-Hadamard transform:
\begin{equation}
    \Omega = \sqrt{\frac{n}{\ell}} D H S
\end{equation}
where $H$ is the Hadamard matrix (requires $n$ to be a power of 2, or padding).

\begin{itemize}
    \item \textbf{Complexity:} $\BigO(mn \log n)$ using Fast Walsh-Hadamard Transform
    \item \textbf{Accuracy:} Similar to SRFT
    \item \textbf{Implementation:} Pure integer/real arithmetic; no complex numbers
\end{itemize}

\subsubsection{CountSketch (Sparse Embeddings)}

For sparse matrices, CountSketch \citep{woodruff2014sketching} uses an extremely sparse $\Omega$:
\begin{equation}
    \Omega_{ij} = \begin{cases}
        +1 & \text{if } h(i) = j \text{ and } s(i) = +1 \\
        -1 & \text{if } h(i) = j \text{ and } s(i) = -1 \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $h: [n] \to [\ell]$ is a random hash function and $s: [n] \to \{+1, -1\}$ is a random sign function.

\begin{itemize}
    \item \textbf{Complexity:} $\BigO(\nnz(A) \cdot \ell)$ --- depends on sparsity, not dimensions
    \item \textbf{Accuracy:} Requires larger $\ell$ than Gaussian but still practical
    \item \textbf{Implementation:} Sparse matrix-matrix multiplication
\end{itemize}

\subsection{Accuracy Guarantees}

The fundamental accuracy result from \citet{halko2011finding} states:

\begin{theorem}[Halko et al., 2011]
Let $A \in \R^{m \times n}$ with singular values $\sigma_1 \geq \sigma_2 \geq \cdots$. For a Gaussian test matrix $\Omega \in \R^{n \times (k+p)}$ with $p \geq 2$, the rank-$k$ approximation $\tilde{A}_k$ satisfies
\begin{equation}
    \E\left[ \frobnorm{A - \tilde{A}_k}^2 \right] \leq \left(1 + \frac{k}{p-1}\right) \sum_{j > k} \sigma_j^2
\end{equation}
and with $q$ power iterations:
\begin{equation}
    \E\left[ \frobnorm{A - \tilde{A}_k}^2 \right] \leq \left(1 + \frac{k}{p-1}\right)^{1/(2q+1)} \left(\sum_{j > k} \sigma_j^{4q+2}\right)^{1/(2q+1)}
\end{equation}
\end{theorem}

\begin{remark}
Power iterations dramatically improve accuracy when the singular value spectrum decays slowly. Each power iteration ``raises'' the spectrum to a higher power, effectively creating a faster-decaying spectrum.
\end{remark}

\subsection{Complexity Summary}

Table~\ref{tab:complexity} summarizes the computational complexity of forming the sketch $Y = A\Omega$ for each method.

\begin{table}[h]
\centering
\caption{Complexity of sketch formation $Y = A\Omega$ for $A \in \R^{m \times n}$, sketch size $\ell$}
\label{tab:complexity}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Complexity} & \textbf{Notes} \\
\midrule
Gaussian & $\BigO(mn\ell)$ & Optimal BLAS; no structure \\
SRFT & $\BigO(mn \log n)$ & FFT-based; complex arithmetic \\
SRHT & $\BigO(mn \log n)$ & FWHT; real arithmetic only \\
CountSketch & $\BigO(\nnz(A) \cdot \ell)$ & Sparse only; scales with nonzeros \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% SECTION 3: IMPLEMENTATION
% ============================================================================
\section{Implementation}
\label{sec:implementation}

A key contribution of this work is providing \emph{both} optimized and naive pure-Python implementations for each sketching method. This allows us to disentangle two distinct sources of performance differences:

\begin{enumerate}
    \item \textbf{Algorithmic complexity:} The asymptotic scaling predicted by theory (e.g., $\BigO(mn\ell)$ vs.\ $\BigO(mn \log n)$).
    \item \textbf{Implementation constants:} The efficiency of underlying libraries (BLAS, FFT) which can differ by orders of magnitude.
\end{enumerate}

\subsection{Repository Structure}

Our implementation is organized as follows:

\begin{lstlisting}[basicstyle=\ttfamily\small]
src/
  randsvd_algorithm.py   # Main randomized SVD
  structured_sketch.py   # SRFT and SRHT operators
  sparse_sketching.py    # CountSketch and sparse embeddings
  naive_sketching.py     # Pure-Python baselines
  hadamardKernel.py      # C-accelerated Hadamard (optional)
  utils.py               # Spectrum generation, utilities
tests/
  run_benchmark_*.py     # All experimental scripts
figures/                 # Generated plots
\end{lstlisting}

\subsection{Optimized Implementations}

\textbf{Gaussian:} Uses \texttt{numpy.random.randn} and \texttt{numpy.dot}, which dispatch to optimized BLAS (DGEMM).

\textbf{SRFT:} Uses \texttt{numpy.fft.fft} applied to each row, followed by column subsampling.

\textbf{SRHT:} Uses either a C-accelerated Fast Walsh-Hadamard Transform (when available) or a pure NumPy iterative implementation.

\textbf{CountSketch:} Constructs a sparse $\Omega$ using \texttt{scipy.sparse.csr\_matrix} and uses sparse-sparse matrix multiplication.

\subsection{Naive Pure-Python Implementations}

To isolate algorithmic complexity, we implement each method using explicit Python loops with no library optimizations:

\textbf{Naive Gaussian:} Triple nested loop computing $Y_{ij} = \sum_k A_{ik} \Omega_{kj}$.

\textbf{Naive FFT (DFT):} $\BigO(n^2)$ discrete Fourier transform via the definition.

\textbf{Naive FWHT:} $\BigO(n \log n)$ recursive Hadamard using pure Python arithmetic.

\textbf{Naive CountSketch:} Direct column extraction based on hash indices.

These naive implementations are slow but reveal the \emph{true} algorithmic complexity ratios without BLAS confounds.

% ============================================================================
% SECTION 4: EXPERIMENTAL SETUP
% ============================================================================
\section{Experimental Setup}
\label{sec:setup}

\subsection{Test Matrices}

We construct synthetic matrices with controlled spectral decay profiles:

\begin{itemize}
    \item \textbf{Exponential decay:} $\sigma_i = e^{-\alpha i}$ with $\alpha = 0.1$. Fast decay; easy for randomized methods.
    \item \textbf{Polynomial decay:} $\sigma_i = i^{-\beta}$ with $\beta = 1$ or $\beta = 2$. Moderate difficulty.
    \item \textbf{Slow decay:} $\sigma_i = 1/\sqrt{i}$ or flat spectrum with noise. Challenging; benefits from power iterations.
\end{itemize}

Matrix sizes: $n = m \in \{1024, 2048, 4096\}$ for square matrices; $m = 8000, n = 2000$ for tall matrices.

For sparse experiments: density $\in \{0.1\%, 1\%, 5\%, 10\%\}$.

\subsection{Parameters}

\begin{itemize}
    \item Target rank: $k \in \{50, 100, 200\}$
    \item Oversampling: $p \in \{5, 10, 20\}$
    \item Sketch size: $\ell = k + p$
    \item Power iterations: $q \in \{0, 1, 2, 4\}$
    \item Trials: 5--10 random seeds per configuration
\end{itemize}

\subsection{Metrics}

\textbf{Speed:}
\begin{itemize}
    \item Time to form sketch $Y = A\Omega$ (the computational bottleneck)
    \item Total randomized SVD runtime
    \item Speedup factor: $t_{\text{Gaussian}} / t_{\text{structured}}$
\end{itemize}

\textbf{Accuracy:}
\begin{itemize}
    \item Relative Frobenius error: $\frobnorm{A - \tilde{A}_k} / \frobnorm{A - A_k^*}$ where $A_k^*$ is the optimal truncated SVD
    \item Relative spectral error: $\specnorm{A - \tilde{A}_k} / \specnorm{A - A_k^*}$
    \item Singular value errors: $|\sigma_i - \tilde{\sigma}_i| / \sigma_i$ for $i = 1, \ldots, k$
\end{itemize}

\subsection{Hardware}

All experiments run on [TODO: specify hardware]. Python 3.x with NumPy (linked to [TODO: BLAS version]), SciPy, and Matplotlib.

% ============================================================================
% SECTION 5: SPEED EXPERIMENTS
% ============================================================================
\section{Speed Experiments}
\label{sec:speed}

\subsection{Dense Matrices: Optimized Implementations}

Figure~\ref{fig:speed_optimized} shows runtime versus sketch size $\ell$ for optimized implementations on dense matrices.

% TODO: Include figure
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.8\textwidth]{figures/speed_comparison.png}
% \caption{Runtime comparison of optimized implementations on dense matrices.}
% \label{fig:speed_optimized}
% \end{figure}

\textbf{Key observation:} Despite having worse asymptotic complexity, Gaussian sketching is often \emph{faster} than SRFT/SRHT for practical problem sizes due to highly optimized BLAS libraries. The crossover point where structured methods win depends on matrix size and the ratio $\ell / n$.

\subsection{Dense Matrices: Pure-Python (Algorithmic) Comparison}

To isolate the algorithmic complexity advantage, we compare naive pure-Python implementations with no library optimization.

Figure~\ref{fig:speed_naive} shows the ``fair'' algorithmic comparison.

\textbf{Key observation:} In the pure-Python setting, structured sketches show clear advantages:
\begin{itemize}
    \item Naive Gaussian: $\BigO(mn\ell)$ --- linear in $\ell$
    \item Naive SRFT/SRHT: $\BigO(mn \log n)$ --- nearly constant in $\ell$
\end{itemize}

The speedup factor approaches $\ell / \log n$ as predicted by theory.

\subsection{Sparse Matrices: CountSketch}

For sparse matrices, CountSketch provides dramatic speedups over dense methods.

Figure~\ref{fig:speed_sparse} compares CountSketch against Gaussian on matrices of varying density.

\textbf{Key observations:}
\begin{itemize}
    \item CountSketch time scales with $\nnz(A)$, not matrix dimensions
    \item For 0.1\% density, CountSketch achieves 10--100$\times$ speedup
    \item Advantage grows with matrix size and decreasing density
\end{itemize}

Pure-Python comparison confirms the algorithmic advantage is $\BigO(n)$ --- CountSketch is fundamentally $n$ times faster than Gaussian for the sketching step on sparse data.

% ============================================================================
% SECTION 6: ACCURACY EXPERIMENTS
% ============================================================================
\section{Accuracy Experiments}
\label{sec:accuracy}

\subsection{Sketching Method Comparison}

Figure~\ref{fig:accuracy_methods} compares approximation quality across Gaussian, SRFT, and SRHT for different spectral decay profiles.

\textbf{Key observation:} All three sketching methods achieve nearly identical accuracy, validating the theoretical prediction that they differ primarily in computational cost, not approximation quality.

\subsection{Effect of Power Iterations}

Figure~\ref{fig:accuracy_power} shows how power iterations ($q$) affect accuracy for slow-decaying spectra.

\textbf{Key observations:}
\begin{itemize}
    \item For fast decay (exponential), $q=0$ suffices
    \item For slow decay (polynomial), $q=1$ or $q=2$ dramatically improves accuracy
    \item Diminishing returns beyond $q=2$ for most practical cases
    \item ``Burn-in'' phenomenon: first iterations may show limited improvement before accuracy converges
\end{itemize}

\subsection{Effect of Oversampling}

Figure~\ref{fig:accuracy_oversample} shows how oversampling parameter $p$ affects accuracy.

\textbf{Key observation:} Oversampling $p = 5$ to $10$ provides a good balance between computational cost and accuracy. Larger $p$ gives diminishing returns.

\subsection{Sparse Embeddings Accuracy}

Figure~\ref{fig:accuracy_sparse} confirms that CountSketch achieves comparable accuracy to Gaussian sketching for the same sketch size $\ell$.

\textbf{Key observation:} Despite using a much sparser random matrix, CountSketch produces approximations within 1--3\% of the optimal Eckart-Young bound, comparable to Gaussian.

% ============================================================================
% SECTION 7: DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Theory vs.\ Practice}

Our experiments reveal an important gap between theoretical complexity and practical performance:

\begin{enumerate}
    \item \textbf{BLAS dominance:} For dense matrices at practical sizes ($n \lesssim 10000$), highly optimized BLAS makes Gaussian competitive despite its worse asymptotic complexity.
    
    \item \textbf{Algorithmic wins are real:} Pure-Python comparisons confirm the $\BigO(mn\ell)$ vs.\ $\BigO(mn \log n)$ gap exists. It simply gets hidden by implementation constants.
    
    \item \textbf{Sparse is different:} For sparse data, CountSketch provides unambiguous wins because the complexity depends on $\nnz(A)$ rather than $mn$.
\end{enumerate}

\subsection{Practical Recommendations}

Based on our experiments, we offer the following guidelines:

\begin{table}[h]
\centering
\caption{Recommended sketching method by problem characteristics}
\label{tab:recommendations}
\begin{tabular}{lll}
\toprule
\textbf{Data Type} & \textbf{Sketch Size $\ell$} & \textbf{Recommended Method} \\
\midrule
Dense, small--medium & Any & Gaussian (BLAS-optimized) \\
Dense, very large & $\ell \gg \log n$ & SRHT or SRFT \\
Sparse ($\nnz \ll mn$) & Any & CountSketch \\
Slow spectral decay & Any & Add power iterations ($q=1,2$) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Achieving $\BigO(mn \log k)$ Complexity}

The literature suggests that SRFT/SRHT can achieve $\BigO(mn \log k)$ rather than $\BigO(mn \log n)$ complexity through careful implementation that avoids computing the full transform. Our current implementation achieves $\BigO(mn \log n)$. Implementing the $\BigO(mn \log k)$ variant is left for future work.

% ============================================================================
% SECTION 8: CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented a comprehensive empirical study of randomized SVD with multiple sketching methods. Our key findings are:

\begin{enumerate}
    \item \textbf{Algorithmic complexity matters, but so do constants:} Structured sketches (SRFT/SRHT) have provably better asymptotic complexity than Gaussian, but highly optimized BLAS libraries narrow the gap significantly for practical problem sizes.
    
    \item \textbf{Pure-Python comparisons reveal true scaling:} By implementing naive versions of all methods, we confirm the theoretical complexity ratios without library confounds.
    
    \item \textbf{CountSketch wins for sparse data:} For sparse matrices, CountSketch provides dramatic speedups that scale with nonzeros rather than matrix dimensions.
    
    \item \textbf{Power iterations control accuracy:} For slowly decaying spectra, power iterations ($q=1,2$) are essential for achieving near-optimal accuracy.
    
    \item \textbf{All methods achieve similar accuracy:} The choice of sketching method primarily affects speed, not approximation quality.
\end{enumerate}

All code and experiments are available at \url{https://github.com/peterKlivnoy/RandSVD} for full reproducibility.

% ============================================================================
% BIBLIOGRAPHY
% ============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Halko et al.(2011)]{halko2011finding}
N.~Halko, P.-G. Martinsson, and J.~A. Tropp.
\newblock Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.
\newblock \emph{SIAM Review}, 53(2):217--288, 2011.

\bibitem[Liberty et al.(2007)]{liberty2007randomized}
E.~Liberty, F.~Woolfe, P.-G. Martinsson, V.~Rokhlin, and M.~Tygert.
\newblock Randomized algorithms for the low-rank approximation of matrices.
\newblock \emph{Proceedings of the National Academy of Sciences}, 104(51):20167--20172, 2007.

\bibitem[Martinsson \& Tropp(2020)]{martinsson2020randomized}
P.-G. Martinsson and J.~A. Tropp.
\newblock Randomized numerical linear algebra: Foundations and algorithms.
\newblock \emph{Acta Numerica}, 29:403--572, 2020.

\bibitem[Woodruff(2014)]{woodruff2014sketching}
D.~P. Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{Foundations and Trends in Theoretical Computer Science}, 10(1--2):1--157, 2014.

\bibitem[Tropp(2011)]{tropp2011improved}
J.~A. Tropp.
\newblock Improved analysis of the subsampled randomized Hadamard transform.
\newblock \emph{Advances in Adaptive Data Analysis}, 3(01n02):115--126, 2011.

\bibitem[Clarkson \& Woodruff(2017)]{clarkson2017low}
K.~L. Clarkson and D.~P. Woodruff.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock \emph{Journal of the ACM}, 63(6):1--45, 2017.

\bibitem[Rokhlin et al.(2010)]{rokhlin2010randomized}
V.~Rokhlin, A.~Szlam, and M.~Tygert.
\newblock A randomized algorithm for principal component analysis.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 31(3):1100--1124, 2010.

\bibitem[Musco \& Musco(2015)]{musco2015randomized}
C.~Musco and C.~Musco.
\newblock Randomized block Krylov methods for stronger and faster approximate singular value decomposition.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Saibaba(2019)]{saibaba2019randomized}
A.~K. Saibaba.
\newblock Randomized subspace iteration: Analysis of canonical angles and unitarily invariant norms.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 40(1):23--48, 2019.

\end{thebibliography}

\end{document}
