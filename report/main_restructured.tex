% ===========================================================================
% PROPOSED REPORT STRUCTURE - Following your storyline
% ===========================================================================
%
% This document outlines the reorganized structure. The existing content
% can be rearranged to fit this flow.
%
% ===========================================================================

\documentclass[10pt,a4paper]{article}

% ============================================================================
% PACKAGES (same as before)
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=0.9in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{natbib}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{../figures/Potential final pictures/fig4_iterations_synthetic.png}
    \caption{Synthetic convergence}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{../figures/Potential final pictures/fig4_krylov_newsgroups.png}
    \caption{20 Newsgroups convergence}
  \end{subfigure}
    
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{../figures/Potential final pictures/fig4_runtime_newsgroups.png}
    \caption{Runtime vs error (Newsgroups)}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{../figures/Potential final pictures/fig4_runtime_vs_error.png}
    \caption{Pareto frontier}
  \end{subfigure}
  \caption{\textbf{Block Krylov and Simultaneous Iteration: Convergence and Runtime.} All four key results are shown together for direct comparison. Block Krylov consistently outperforms Simultaneous Iteration on strong error metrics and runtime tradeoffs, especially for high-accuracy requirements.}
  \label{fig:block-krylov-quad}
\end{figure}

\subsection{Summary: When to Use What}
% Keep SHORT - 1 paragraph

\subsection{The Computational Challenge}
% Full SVD is O(n^3) - prohibitive for large matrices
% But we often only need rank-k approximation

\subsection{The Randomized SVD Algorithm}
% Present Algorithm 1 here
% Intuition: random projection captures dominant directions
% Two stages: (A) Range finding, (B) Deterministic SVD on small matrix

\begin{algorithm}[t]
\caption{Basic Randomized SVD}
\label{alg:basic-randsvd}
\begin{algorithmic}[1]
\Require Matrix $A \in \R^{m \times n}$, target rank $k$, oversampling $p$
\Ensure Approximate rank-$k$ SVD: $\tilde{U}, \tilde{\Sigma}, \tilde{V}$
\State Set sketch size $\ell = k + p$
\State Draw random test matrix $\Omega \in \R^{n \times \ell}$ (e.g., Gaussian)
\State Form sketch $Y = A\Omega$
\State Compute QR: $Y = QR$
\State Form small matrix $B = Q^T A \in \R^{\ell \times n}$
\State Compute SVD: $B = \hat{U} \Sigma V^T$
\State Set $\tilde{U} = Q\hat{U}$
\State \Return $\tilde{U}_{:,1:k}, \Sigma_{1:k,1:k}, V_{:,1:k}$
\end{algorithmic}
\end{algorithm}

\subsection{Theory: Error Bounds}
% Brief statement of Halko et al. bound
% E[||A - A_k||] <= (1 + k/(p-1)) * optimal

\subsection{Motivating Experiment: Speed vs Accuracy}
% FIGURE 0: The fundamental advantage
% Panel (a): Time comparison - Full SVD vs RandSVD
% Panel (b): Approximation error - shows near-optimal accuracy
% Caption: "RandSVD achieves 100-1000x speedup while maintaining <1% error"

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/fig0_svd_motivation.pdf}
\caption{\textbf{The fundamental advantage of randomized SVD.} 
(a) Computation time comparison showing $\BigO(n^3)$ scaling of full SVD versus 
$\BigO(n^2 k)$ scaling of randomized SVD. 
(b) Approximation error remains within 1\% of optimal across all matrix sizes.
RandSVD achieves speedups of 100-800$\times$ while maintaining near-optimal accuracy.}
\label{fig:motivation}
\end{figure}


% ============================================================================
% SECTION 2: THE ITERATION IMPROVEMENT CHAPTER
% ============================================================================
\section{Improving Accuracy Through Iteration}
\label{sec:iteration}

% This is the CORE motivational chapter - why do we need iterations?

\subsection{The Problem: Spectral Decay Dependence}

The basic randomized SVD algorithm ($q=0$) works well when the singular values of $A$ decay rapidly---there is a clear ``gap'' between the signal we want to capture and the noise we want to discard. However, real-world data often exhibits slowly decaying singular values, where many components contribute meaningfully to the matrix structure.

The fundamental issue is \emph{spectral bias}: when $q=0$, the random projection captures energy from tail singular values, causing the algorithm to \textbf{overestimate} small singular values. This is not merely a theoretical concern---it directly impacts the quality of the low-rank approximation.

% FIGURE 2: Easy vs Hard comparison (THE key motivation)
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig2_easy_vs_hard.png}
\caption{\textbf{Singular value recovery: Easy vs Hard data.}
(Left) Fast-decay matrix ($\sigma_i = 0.9^i$): all methods achieve near-perfect recovery.
(Right) Slow-decay matrix ($\sigma_i = 1/\sqrt{i}$): without power iterations ($q=0$), 
recovered singular values drift \emph{above} the true values (spectral bias). 
With $q=1$ and $q=2$, the algorithm ``snaps'' onto the true spectrum.
This plot visualizes exactly \emph{where} the error comes from and \emph{why} iterations help.}
\label{fig:easy-vs-hard}
\end{figure}

Figure~\ref{fig:easy-vs-hard} demonstrates this phenomenon directly. On a fast-decay matrix (left panel), all methods---$q=0$, $q=1$, $q=2$---achieve essentially perfect singular value recovery. The true singular values (black dashed line) are indistinguishable from the recovered values. This is the ``easy'' case where basic randomized SVD excels.

The right panel tells a different story. On a slow-decay matrix with $\sigma_i = 1/\sqrt{i}$, the $q=0$ curve (blue) clearly drifts \emph{above} the true values, particularly for the tail singular values. This spectral bias means the algorithm is capturing noise energy and attributing it to signal. With just one power iteration ($q=1$), the recovered values snap back onto the true spectrum. With $q=2$, the recovery is essentially perfect.

\subsection{Solution 1: Power Iteration (Simultaneous Subspace Iteration)}

The classical solution is \emph{power iteration}, also known as simultaneous subspace iteration. Instead of computing $Y = A\Omega$, we compute:
\begin{equation}
Y = (AA^T)^q A\Omega
\end{equation}
This has the effect of raising the singular values to the $(2q+1)$-th power before the randomized range finding step. If the original singular values are $\sigma_1, \sigma_2, \ldots$, the effective singular values become $\sigma_1^{2q+1}, \sigma_2^{2q+1}, \ldots$. For slowly decaying spectra, this dramatically increases the gap between the top-$k$ singular values and the rest.

\begin{algorithm}[t]
\caption{Randomized SVD with Power Iterations}
\label{alg:power-iter}
\begin{algorithmic}[1]
\Require Matrix $A \in \R^{m \times n}$, rank $k$, oversampling $p$, power iterations $q$
\State Draw $\Omega \in \R^{n \times (k+p)}$ with i.i.d. Gaussian entries
\State $Y \gets A\Omega$
\For{$j = 1, \ldots, q$}
    \State $\tilde{Y} \gets A^T Y$; \quad $[\tilde{Q}, \sim] \gets \texttt{qr}(\tilde{Y})$ \Comment{Orthogonalize for stability}
    \State $Y \gets A\tilde{Q}$; \quad $[Q, \sim] \gets \texttt{qr}(Y)$
\EndFor
\State Continue with standard algorithm (project onto $Q$, compute small SVD)
\end{algorithmic}
\end{algorithm}

The theoretical improvement is captured by the error bound. Without power iterations:
\begin{equation}
\E\left[ \frobnorm{A - \tilde{A}_k}^2 \right] \leq \left(1 + \frac{k}{p-1}\right) \sum_{j > k} \sigma_j^2
\end{equation}
With $q$ power iterations:
\begin{equation}
\E\left[ \frobnorm{A - \tilde{A}_k}^2 \right] \leq \left(1 + \frac{k}{p-1}\right)^{1/(2q+1)} \left(\sum_{j > k} \sigma_j^{4q+2}\right)^{1/(2q+1)}
\end{equation}
The exponent $4q+2$ in the tail sum means that tail singular values are suppressed exponentially with $q$.

\subsection{Solution 2: Block Krylov Methods}

A more sophisticated approach, developed by \citet{musco2015randomized}, observes that power iteration \emph{discards} intermediate information. After computing $A\Omega$, $A^TA\Omega$, $(A^TA)^2\Omega$, etc., standard power iteration keeps only the final result. Block Krylov methods instead \emph{retain the entire sequence}:
\begin{equation}
\mathcal{K}_q(A^TA, A\Omega) = \text{span}\{A\Omega, \; A^TA \cdot A\Omega, \; (A^TA)^2 \cdot A\Omega, \; \ldots, \; (A^TA)^q \cdot A\Omega\}
\end{equation}

This Krylov subspace contains richer spectral information than the final power iteration result alone. The key theoretical advantage is \textbf{gap-independent convergence}: while simultaneous iteration convergence depends on the ratio $\sigma_k/\sigma_{k+1}$ (slow when singular values are closely spaced), Block Krylov convergence is largely independent of spectral gaps.

\begin{algorithm}[t]
\caption{Block Krylov SVD}
\label{alg:krylov}
\begin{algorithmic}[1]
\Require Matrix $A \in \R^{m \times n}$, rank $k$, block size $\ell$, Krylov depth $q$
\State Draw $\Omega \in \R^{n \times \ell}$
\State $K_0 \gets A\Omega$
\For{$j = 1, \ldots, q$}
    \State $K_j \gets A(A^T K_{j-1})$ \Comment{Apply $AA^T$ to previous block}
\EndFor
\State $K \gets [K_0 \mid K_1 \mid \cdots \mid K_q]$ \Comment{Concatenate all blocks}
\State $Q \gets \texttt{orth}(K)$ \Comment{Orthonormalize the Krylov basis}
\State $B \gets Q^T A$; \quad compute SVD of $B$
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Comparison: Synthetic Data}

Figure~\ref{fig:iterations-synthetic} compares Block Krylov and Simultaneous Iteration on synthetic matrices with controlled spectral decay. We measure three increasingly stringent error metrics following \citet{musco2015randomized}:

\begin{itemize}
\item \textbf{Frobenius error} (weak): $\|A - ZZ^TA\|_F / \|A - A_k\|_F - 1$
\item \textbf{Spectral error} (strong): $\|A - ZZ^TA\|_2 / \|A - A_k\|_2 - 1$  
\item \textbf{Per-vector error} (strongest): $\max_i |\sigma_i^2 - \|A^Tz_i\|^2| / \sigma_{k+1}^2$
\end{itemize}

The per-vector metric is particularly important for PCA applications: it ensures each approximate singular vector captures the correct amount of variance.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig4_iterations_synthetic.png}
\caption{\textbf{Convergence comparison on synthetic data.}
Block Krylov (red) vs Simultaneous Iteration (blue) across three error metrics.
Both methods perform similarly on the weak Frobenius metric, but Block Krylov shows
dramatic advantages on the stronger spectral and per-vector metrics, especially
at low iteration counts.}
\label{fig:iterations-synthetic}
\end{figure}

\subsection{Experimental Comparison: Real Data (20 Newsgroups)}

To validate on real-world data, we use the 20 Newsgroups text corpus---a standard benchmark with naturally heavy-tailed singular value distribution. Figure~\ref{fig:krylov-newsgroups} shows the convergence behavior.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig4_krylov_newsgroups.png}
\caption{\textbf{Convergence on 20 Newsgroups data.}
Real text data exhibits slow spectral decay, making this a challenging test case.
Block Krylov achieves lower error at each iteration count, with the advantage
most pronounced on the per-vector metric.}
\label{fig:krylov-newsgroups}
\end{figure}

\subsection{Runtime Considerations}

Block Krylov has overhead compared to simultaneous iteration: the orthogonalization step operates on a basis of size $\ell(q+1)$ rather than $\ell$. However, when comparing at \emph{equal accuracy} rather than equal iteration count, Block Krylov often wins.

Figure~\ref{fig:runtime-newsgroups} shows error versus wall-clock runtime on the 20 Newsgroups dataset.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig4_runtime_newsgroups.png}
\caption{\textbf{Runtime vs error on 20 Newsgroups.}
When accounting for actual computation time, Block Krylov achieves better
error-runtime tradeoffs, reaching target accuracy levels faster despite
the per-iteration overhead.}
\label{fig:runtime-newsgroups}
\end{figure}

Figure~\ref{fig:runtime-vs-error} provides a comprehensive view of the runtime-accuracy tradeoff across different parameter settings.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig4_runtime_vs_error.png}
\caption{\textbf{Runtime vs error tradeoff.}
Each point represents a different parameter configuration.
The Pareto frontier shows the best achievable error for each runtime budget.
Block Krylov dominates for high-accuracy requirements; simultaneous iteration
is competitive for lower accuracy targets where fewer iterations suffice.}
\label{fig:runtime-vs-error}
\end{figure}

\subsection{Summary: When to Use What}

Based on our experiments and theory:

\begin{itemize}
\item \textbf{$q=0$ (no iterations)}: Appropriate only when the spectrum is known to decay rapidly (e.g., images, signals with clear low-rank structure). Fast but may have significant spectral bias on hard data.

\item \textbf{$q=1$--$2$ (power iteration)}: The robust default for unknown data. Provides substantial accuracy improvement with modest computational overhead. Usually sufficient for Frobenius-norm applications.

\item \textbf{Block Krylov}: Preferred when:
  \begin{itemize}
  \item Per-vector accuracy matters (PCA where individual components are interpreted)
  \item Data has heavy-tailed spectrum with small spectral gaps
  \item Number of passes over data is constrained (streaming/out-of-core)
  \item Spectral-norm guarantees are needed
  \end{itemize}
\end{itemize}

The key insight is that the ``right'' method depends on which error metric matters for your application. Frobenius error (average reconstruction quality) is forgiving; per-vector error (individual component quality) is demanding. Choose accordingly.


% ============================================================================
% SECTION 3: THE SKETCHING CHAPTER
% ============================================================================
\section{Efficient Sketching Methods}
\label{sec:sketching}

Having established how to achieve accurate low-rank approximations through iteration, we now turn to computational efficiency. The dominant cost in randomized SVD is the sketch formation $Y = A\Omega$. This section explores how different choices of $\Omega$ affect runtime.

\subsection{The Computational Bottleneck}

For a matrix $A \in \R^{m \times n}$ and sketch size $\ell$, the basic Gaussian sketch $Y = A\Omega$ requires $\BigO(mn\ell)$ operations---a dense matrix-matrix multiplication. This is the same asymptotic cost as $\ell$ matrix-vector products, and for large $\ell$, it can dominate the overall computation.

The key question: can we design structured random matrices that achieve the same statistical properties as Gaussian while requiring fewer operations?

\subsection{Structured Random Transforms for Dense Matrices}

The answer is yes. The Subsampled Randomized Fourier Transform (SRFT) and Subsampled Randomized Hadamard Transform (SRHT) achieve $\BigO(mn \log n)$ complexity---independent of sketch size $\ell$.

\subsubsection{Subsampled Randomized Fourier Transform (SRFT)}

The SRFT test matrix has the form:
\begin{equation}
\Omega = \sqrt{\frac{n}{\ell}} \cdot D F^* S
\end{equation}
where $D$ is a diagonal matrix of random signs ($\pm 1$), $F$ is the discrete Fourier transform matrix, and $S$ samples $\ell$ columns uniformly at random. The key insight is that $F^*$ can be applied via the Fast Fourier Transform in $\BigO(n \log n)$ per row, giving total complexity $\BigO(mn \log n)$.

\subsubsection{Subsampled Randomized Hadamard Transform (SRHT)}

The SRHT replaces the Fourier transform with the Walsh-Hadamard transform:
\begin{equation}
\Omega = \sqrt{\frac{n}{\ell}} \cdot D H S
\end{equation}
where $H$ is the Hadamard matrix. The Fast Walsh-Hadamard Transform (FWHT) uses only additions and subtractions---no complex arithmetic or trigonometric functions---making it particularly efficient. The recursive structure is:
\begin{equation}
H_{2^k} = \begin{bmatrix} H_{2^{k-1}} & H_{2^{k-1}} \\ H_{2^{k-1}} & -H_{2^{k-1}} \end{bmatrix}, \quad H_1 = [1]
\end{equation}

Both SRFT and SRHT satisfy the Johnson-Lindenstrauss property and achieve similar accuracy guarantees to Gaussian projections, with the same oversampling requirements.

\subsection{Experimental Results: Dense Matrices}

Figure~\ref{fig:dense-speed} compares the three sketching methods on dense matrices.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig1_speed_dense.png}
\caption{\textbf{Sketching speed comparison on dense matrices.}
Comparison of Gaussian, SRFT, and SRHT across different matrix sizes and sketch sizes.
While theory predicts SRFT/SRHT should be faster for large $\ell$, highly optimized 
BLAS libraries make Gaussian surprisingly competitive in practice.}
\label{fig:dense-speed}
\end{figure}

\subsection{Discussion: Theory vs Practice}

A key finding is the gap between theoretical complexity and practical performance. In theory, SRFT and SRHT should dominate for large sketch sizes due to their $\BigO(mn \log n)$ vs $\BigO(mn\ell)$ complexity. In practice:

\begin{itemize}
\item \textbf{BLAS optimization}: Modern BLAS implementations (OpenBLAS, MKL, Accelerate) achieve near-peak throughput for dense matrix multiplication. The DGEMM routine is among the most optimized code in scientific computing.

\item \textbf{FFT/FWHT overhead}: While asymptotically superior, FFT and Hadamard implementations carry larger constants and may not be as finely tuned as BLAS.

\item \textbf{Crossover point}: For matrices up to dimension $\sim 10^4$ with moderate sketch sizes, Gaussian sketching with BLAS is often competitive or faster.
\end{itemize}

The practical recommendation: for moderate-sized dense matrices, Gaussian sketching is simple and effective. The theoretical advantages of structured sketches become relevant for very large matrices or in environments without optimized BLAS.

\subsection{Sparse Matrices: CountSketch and Sparse Sign}

For sparse matrices, the situation is fundamentally different. Applying a dense Gaussian $\Omega$ to a sparse $A$ destroys the sparsity structure, requiring $\BigO(mn\ell)$ operations regardless of how few nonzeros $A$ contains.

\textbf{CountSketch} solves this with an extremely sparse test matrix. Each column of $\Omega$ contains exactly \emph{one} nonzero entry:
\begin{equation}
\Omega_{h(j), j} = s(j), \quad \text{all other entries } 0
\end{equation}
where $h: [n] \to [\ell]$ is a random hash function and $s: [n] \to \{+1, -1\}$ is a random sign function. The sketch $Y = A\Omega$ can be computed by iterating over the nonzeros of $A$ once, accumulating into the appropriate rows of $Y$. Total complexity: $\BigO(\nnz(A))$---independent of matrix dimensions and sketch size!

\textbf{Sparse Sign Embedding} generalizes CountSketch by allowing $s > 1$ nonzeros per column, trading off sparsity for improved concentration properties.

\subsection{Experimental Results: Sparse Matrices}

Figure~\ref{fig:sparse-speed} demonstrates the dramatic advantage of CountSketch on sparse data.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/Potential final pictures/fig2_speed_sparse.png}
\caption{\textbf{Sketching speed on sparse matrices.}
CountSketch achieves speedups of 100--300$\times$ compared to Gaussian at low densities.
The key: CountSketch complexity scales with $\nnz(A)$, not matrix dimensions.
At 0.1\% density, this represents a 1000$\times$ reduction in effective matrix size.}
\label{fig:sparse-speed}
\end{figure}

The speedup is dramatic: at 0.1\% density, CountSketch is over 100$\times$ faster than Gaussian. The scaling confirms the theoretical prediction: Gaussian time is nearly constant across density levels (depends on dimensions), while CountSketch time scales linearly with density (depends on $\nnz$).

\subsection{Speed-Accuracy Tradeoffs: The Pareto Frontier}

Different sketching methods offer different speed-accuracy tradeoffs. Figure~\ref{fig:pareto} shows the Pareto frontier---the best achievable accuracy for each runtime budget.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{../figures/Potential final pictures/fig3_pareto_sketching_time.png}
\caption{\textbf{Speed vs accuracy Pareto frontier.}
Each curve shows how approximation error decreases as more time is invested in sketching.
The Pareto frontier (lower-left envelope) represents the best tradeoffs.
Different methods dominate in different regimes.}
\label{fig:pareto}
\end{figure}

\subsection{Summary: Choosing a Sketching Method}

Based on our experiments:

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Scenario} & \textbf{Recommended Method} \\
\midrule
Dense matrix, $n < 10^4$ & Gaussian (simple, BLAS-optimized) \\
Dense matrix, $n > 10^4$ & SRHT (asymptotic advantage) \\
Sparse matrix, any size & CountSketch (scales with $\nnz$) \\
Very large $\ell$ needed & SRFT/SRHT ($\ell$-independent) \\
\bottomrule
\end{tabular}
\end{center}

The key insight: \textbf{choose the sketching method for speed; use iterations to control accuracy.} All methods achieve similar approximation quality given sufficient power iterations---the difference is purely computational.


% ============================================================================
% SECTION 4: APPLICATION - VIDEO BACKGROUND SEPARATION
% ============================================================================
\section{Application: Video Background Separation}
\label{sec:application}

% [PLACEHOLDER - Your video demonstration goes here]

\subsection{Problem Setup}
% Video as matrix: columns = frames, rows = pixels
% Low-rank component = static background
% Sparse component = moving foreground

\subsection{Method}
% Apply randomized SVD to extract low-rank approximation
% Background = low-rank, Foreground = residual

\subsection{Results}
% [Your figures/videos go here]
% Show: Original, Background, Foreground

\begin{figure}[t]
\centering
% \includegraphics[width=\textwidth]{../figures/video_separation.pdf}
\fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}
[Video Background Separation Results - To Be Added]
\vspace{2cm}}}
\caption{\textbf{Video background separation using randomized SVD.}
[Description of your results]}
\label{fig:video}
\end{figure}


% ============================================================================
% SECTION 5: CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Contributions}
% 1. Comprehensive empirical study of RandSVD
% 2. Motivation for power iterations via singular value recovery
% 3. Comparison of Block Krylov vs Simultaneous Iteration
% 4. Practical guidance on sketching method selection

\subsection{Practical Recommendations}
% Default: Gaussian + q=1 + p=10
% Sparse data: CountSketch
% Heavy tails / PCA: Block Krylov
% Check for outliers first!

\subsection{Future Directions}
% Streaming/online settings
% Tensor extensions
% Robust methods


% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Halko et al.(2011)]{halko2011finding}
N.~Halko, P.-G. Martinsson, and J.~A. Tropp.
\newblock Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.
\newblock \emph{SIAM Review}, 53(2):217--288, 2011.

\bibitem[Musco \& Musco(2015)]{musco2015randomized}
C.~Musco and C.~Musco.
\newblock Randomized block Krylov methods for stronger and faster approximate singular value decomposition.
\newblock \emph{NeurIPS}, 2015.

\bibitem[Martinsson \& Tropp(2020)]{martinsson2020randomized}
P.-G. Martinsson and J.~A. Tropp.
\newblock Randomized numerical linear algebra: Foundations and algorithms.
\newblock \emph{Acta Numerica}, 29:403--572, 2020.

\bibitem[Clarkson \& Woodruff(2017)]{clarkson2017low}
K.~L. Clarkson and D.~P. Woodruff.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock \emph{Journal of the ACM}, 63(6):1--45, 2017.

\bibitem[Woodruff(2014)]{woodruff2014sketching}
D.~P. Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{Foundations and Trends in TCS}, 10(1--2):1--157, 2014.

\end{thebibliography}

% ============================================================================
% APPENDIX (Optional)
% ============================================================================
\appendix

\section{Implementation Details}
% Code structure, library versions, etc.

\section{Additional Experiments}
% Robustness analysis, parameter sensitivity, etc.

\end{document}
