\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Basic Randomized SVD}}{2}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:basic-randsvd}{{1}{2}{Basic Randomized SVD}{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {The fundamental advantage of randomized SVD.} (a) Computation time comparison showing $\mathcal  {O}(n^3)$ scaling of full SVD versus $\mathcal  {O}(n^2 k)$ scaling of randomized SVD. (b) Approximation error remains within 1\% of optimal across all matrix sizes. RandSVD achieves speedups of 100-800$\times $ while maintaining near-optimal accuracy.}}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig:motivation}{{1}{2}{\textbf {The fundamental advantage of randomized SVD.} (a) Computation time comparison showing $\BigO (n^3)$ scaling of full SVD versus $\BigO (n^2 k)$ scaling of randomized SVD. (b) Approximation error remains within 1\% of optimal across all matrix sizes. RandSVD achieves speedups of 100-800$\times $ while maintaining near-optimal accuracy}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The SVD and Its Importance}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The Computational Challenge}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The Randomized SVD Algorithm}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Theory: Error Bounds}{2}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Motivating Experiment: Speed vs Accuracy}{2}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Improving Accuracy Through Iteration}{2}{section.2}\protected@file@percent }
\newlabel{sec:iteration}{{2}{2}{Improving Accuracy Through Iteration}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Problem: Spectral Decay Dependence}{2}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Singular value recovery: Easy vs Hard data.} (Left) Fast-decay matrix ($\sigma _i = 0.9^i$): all methods achieve near-perfect recovery. (Right) Slow-decay matrix ($\sigma _i = 1/\sqrt  {i}$): without power iterations ($q=0$), recovered singular values drift \emph  {above} the true values (spectral bias). With $q=1$ and $q=2$, the algorithm ``snaps'' onto the true spectrum. This plot visualizes exactly \emph  {where} the error comes from and \emph  {why} iterations help.}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:easy-vs-hard}{{2}{3}{\textbf {Singular value recovery: Easy vs Hard data.} (Left) Fast-decay matrix ($\sigma _i = 0.9^i$): all methods achieve near-perfect recovery. (Right) Slow-decay matrix ($\sigma _i = 1/\sqrt {i}$): without power iterations ($q=0$), recovered singular values drift \emph {above} the true values (spectral bias). With $q=1$ and $q=2$, the algorithm ``snaps'' onto the true spectrum. This plot visualizes exactly \emph {where} the error comes from and \emph {why} iterations help}{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Randomized SVD with Power Iterations}}{3}{algorithm.2}\protected@file@percent }
\newlabel{alg:power-iter}{{2}{3}{Randomized SVD with Power Iterations}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Solution 1: Power Iteration (Simultaneous Subspace Iteration)}{3}{subsection.2.2}\protected@file@percent }
\citation{musco2015randomized}
\citation{musco2015randomized}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Block Krylov SVD}}{4}{algorithm.3}\protected@file@percent }
\newlabel{alg:krylov}{{3}{4}{Block Krylov SVD}{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Solution 2: Block Krylov Methods}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Experimental Comparison: Synthetic Data}{4}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Experimental Comparison: Real Data (20 Newsgroups)}{4}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Runtime Considerations}{4}{subsection.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Convergence comparison on synthetic data.} Block Krylov (red) vs Simultaneous Iteration (blue) across three error metrics. Both methods perform similarly on the weak Frobenius metric, but Block Krylov shows dramatic advantages on the stronger spectral and per-vector metrics, especially at low iteration counts.}}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:iterations-synthetic}{{3}{5}{\textbf {Convergence comparison on synthetic data.} Block Krylov (red) vs Simultaneous Iteration (blue) across three error metrics. Both methods perform similarly on the weak Frobenius metric, but Block Krylov shows dramatic advantages on the stronger spectral and per-vector metrics, especially at low iteration counts}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Summary: When to Use What}{5}{subsection.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Convergence on 20 Newsgroups data.} Real text data exhibits slow spectral decay, making this a challenging test case. Block Krylov achieves lower error at each iteration count, with the advantage most pronounced on the per-vector metric.}}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:krylov-newsgroups}{{4}{6}{\textbf {Convergence on 20 Newsgroups data.} Real text data exhibits slow spectral decay, making this a challenging test case. Block Krylov achieves lower error at each iteration count, with the advantage most pronounced on the per-vector metric}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Efficient Sketching Methods}{6}{section.3}\protected@file@percent }
\newlabel{sec:sketching}{{3}{6}{Efficient Sketching Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Computational Bottleneck}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Structured Random Transforms for Dense Matrices}{6}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Runtime vs error on 20 Newsgroups.} When accounting for actual computation time, Block Krylov achieves better error-runtime tradeoffs, reaching target accuracy levels faster despite the per-iteration overhead.}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:runtime-newsgroups}{{5}{7}{\textbf {Runtime vs error on 20 Newsgroups.} When accounting for actual computation time, Block Krylov achieves better error-runtime tradeoffs, reaching target accuracy levels faster despite the per-iteration overhead}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Subsampled Randomized Fourier Transform (SRFT)}{7}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Subsampled Randomized Hadamard Transform (SRHT)}{7}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Runtime vs error tradeoff.} Each point represents a different parameter configuration. The Pareto frontier shows the best achievable error for each runtime budget. Block Krylov dominates for high-accuracy requirements; simultaneous iteration is competitive for lower accuracy targets where fewer iterations suffice.}}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:runtime-vs-error}{{6}{8}{\textbf {Runtime vs error tradeoff.} Each point represents a different parameter configuration. The Pareto frontier shows the best achievable error for each runtime budget. Block Krylov dominates for high-accuracy requirements; simultaneous iteration is competitive for lower accuracy targets where fewer iterations suffice}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experimental Results: Dense Matrices}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion: Theory vs Practice}{8}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Sketching speed comparison on dense matrices.} Comparison of Gaussian, SRFT, and SRHT across different matrix sizes and sketch sizes. While theory predicts SRFT/SRHT should be faster for large $\ell $, highly optimized BLAS libraries make Gaussian surprisingly competitive in practice.}}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:dense-speed}{{7}{9}{\textbf {Sketching speed comparison on dense matrices.} Comparison of Gaussian, SRFT, and SRHT across different matrix sizes and sketch sizes. While theory predicts SRFT/SRHT should be faster for large $\ell $, highly optimized BLAS libraries make Gaussian surprisingly competitive in practice}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Sparse Matrices: CountSketch and Sparse Sign}{9}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Experimental Results: Sparse Matrices}{9}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Speed-Accuracy Tradeoffs: The Pareto Frontier}{9}{subsection.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Summary: Choosing a Sketching Method}{9}{subsection.3.8}\protected@file@percent }
\bibstyle{plainnat}
\bibcite{halko2011finding}{{1}{2011}{{Halko et al.}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Sketching speed on sparse matrices.} CountSketch achieves speedups of 100--300$\times $ compared to Gaussian at low densities. The key: CountSketch complexity scales with $\mathrm  {nnz}(A)$, not matrix dimensions. At 0.1\% density, this represents a 1000$\times $ reduction in effective matrix size.}}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:sparse-speed}{{8}{10}{\textbf {Sketching speed on sparse matrices.} CountSketch achieves speedups of 100--300$\times $ compared to Gaussian at low densities. The key: CountSketch complexity scales with $\nnz (A)$, not matrix dimensions. At 0.1\% density, this represents a 1000$\times $ reduction in effective matrix size}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Speed vs accuracy Pareto frontier.} Each curve shows how approximation error decreases as more time is invested in sketching. The Pareto frontier (lower-left envelope) represents the best tradeoffs. Different methods dominate in different regimes.}}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pareto}{{9}{11}{\textbf {Speed vs accuracy Pareto frontier.} Each curve shows how approximation error decreases as more time is invested in sketching. The Pareto frontier (lower-left envelope) represents the best tradeoffs. Different methods dominate in different regimes}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Video background separation using randomized SVD.} [Description of your results]}}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig:video}{{10}{11}{\textbf {Video background separation using randomized SVD.} [Description of your results]}{figure.caption.11}{}}
\bibcite{musco2015randomized}{{2}{2015}{{Musco \& Musco}}{{}}}
\bibcite{martinsson2020randomized}{{3}{2020}{{Martinsson \& Tropp}}{{}}}
\bibcite{clarkson2017low}{{4}{2017}{{Clarkson \& Woodruff}}{{}}}
\bibcite{woodruff2014sketching}{{5}{2014}{{Woodruff}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Application: Video Background Separation}{12}{section.4}\protected@file@percent }
\newlabel{sec:application}{{4}{12}{Application: Video Background Separation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Problem Setup}{12}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Method}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{12}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Conclusion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Summary of Contributions}{12}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Practical Recommendations}{12}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Future Directions}{12}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementation Details}{12}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Experiments}{12}{appendix.B}\protected@file@percent }
\gdef \@abspage@last{12}
